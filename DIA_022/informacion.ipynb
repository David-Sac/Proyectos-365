{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIA 022: Optimización de Rendimiento y Escalabilidad de la API de Predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Objetivos del Día 22\n",
    "Optimizar el Rendimiento de la API:\n",
    "\n",
    "Implementar mecanismos de caching para reducir la carga del servidor y disminuir la latencia.\n",
    "Optimizar el modelo de machine learning para mejorar los tiempos de inferencia.\n",
    "Configurar la Escalabilidad en Heroku:\n",
    "\n",
    "Ajustar la configuración de dynos para manejar incrementos en el tráfico.\n",
    "Implementar auto-scaling si la plataforma lo permite.\n",
    "Mejorar la Gestión de Logs y Monitoreo:\n",
    "\n",
    "Refinar las herramientas de logging y monitoreo para una mejor visibilidad del rendimiento de la API.\n",
    "2. Optimizar el Rendimiento de la API\n",
    "2.1. Implementar Caching con Redis\n",
    "El caching almacena las respuestas a solicitudes frecuentes, reduciendo la necesidad de recalcular predicciones y disminuyendo la latencia.\n",
    "\n",
    "2.1.1. Instalar Redis\n",
    "Agregar Redis a requirements.txt:\n",
    "\n",
    "plaintext\n",
    "Copiar\n",
    "redis==4.5.5\n",
    "Actualizar las Dependencias:\n",
    "\n",
    "bash\n",
    "Copiar\n",
    "pip install -r requirements.txt\n",
    "Configurar Redis en Heroku:\n",
    "\n",
    "Añadir el Add-on de Redis:\n",
    "bash\n",
    "Copiar\n",
    "heroku addons:create heroku-redis:hobby-dev --app mnist-flask-api\n",
    "Obtener la URL de Redis:\n",
    "bash\n",
    "Copiar\n",
    "heroku config:get REDIS_URL --app mnist-flask-api\n",
    "2.1.2. Integrar Redis en api.py\n",
    "Actualiza tu archivo api.py para utilizar Redis como sistema de caching.\n",
    "\n",
    "python\n",
    "Copiar\n",
    "# api.py\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import redis\n",
    "import hashlib\n",
    "from functools import wraps\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configurar Redis\n",
    "redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379/0')\n",
    "cache = redis.Redis.from_url(redis_url)\n",
    "\n",
    "# Cargar el modelo optimizado\n",
    "modelo_web = tf.keras.models.load_model('modelo_cnn_transfer_learning_mnist_final_optimizado.h5')\n",
    "\n",
    "# Obtener la API Key desde variables de entorno\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "def require_api_key(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        key = request.headers.get('x-api-key')\n",
    "        if key and key == API_KEY:\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            return jsonify({'error': 'Unauthorized access'}), 401\n",
    "    return wrapper\n",
    "\n",
    "def allowed_file(filename):\n",
    "    ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "def generate_cache_key(file_bytes):\n",
    "    return hashlib.sha256(file_bytes).hexdigest()\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "@require_api_key\n",
    "def predict_web():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No se encontró el archivo en la solicitud.'}), 400\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No se seleccionó ningún archivo.'}), 400\n",
    "    if not allowed_file(file.filename):\n",
    "        return jsonify({'error': 'Tipo de archivo no permitido. Por favor, sube una imagen PNG, JPG, JPEG o GIF.'}), 400\n",
    "    try:\n",
    "        # Leer los bytes del archivo para generar la clave de cache\n",
    "        file_bytes = file.read()\n",
    "        cache_key = generate_cache_key(file_bytes)\n",
    "        \n",
    "        # Verificar si la predicción ya está en cache\n",
    "        cached_result = cache.get(cache_key)\n",
    "        if cached_result:\n",
    "            prediccion, probabilidad = cached_result.decode('utf-8').split(',')\n",
    "            return jsonify({'prediccion': int(prediccion), 'probabilidad': float(probabilidad)})\n",
    "\n",
    "        # Procesar la imagen\n",
    "        img = Image.open(io.BytesIO(file_bytes)).convert('L')  # Convertir a escala de grises\n",
    "        img = img.resize((28, 28))  # Redimensionar a 28x28\n",
    "        img_array = np.array(img).astype('float32') / 255.0  # Normalizar\n",
    "        img_array = img_array.reshape((1, 28, 28, 1))  # Añadir dimensión de batch\n",
    "\n",
    "        # Preprocesar para el modelo (redimensionar a 224x224 y convertir a RGB)\n",
    "        img_resized = tf.image.resize(img_array, [224, 224])\n",
    "        img_rgb = tf.image.grayscale_to_rgb(img_resized)\n",
    "\n",
    "        # Realizar la predicción\n",
    "        pred = modelo_web.predict(img_rgb)\n",
    "        pred_class = np.argmax(pred, axis=1)[0]\n",
    "        pred_prob = float(np.max(pred))\n",
    "\n",
    "        # Almacenar el resultado en cache por 1 hora\n",
    "        cache.setex(cache_key, 3600, f\"{pred_class},{pred_prob}\")\n",
    "\n",
    "        return jsonify({'prediccion': int(pred_class), 'probabilidad': pred_prob})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "Explicación:\n",
    "\n",
    "Integración con Redis:\n",
    "Conectamos a Redis utilizando la URL proporcionada por Heroku.\n",
    "Creamos una función generate_cache_key que genera una clave única basada en los bytes del archivo.\n",
    "Caching de Resultados:\n",
    "Antes de procesar la imagen, verificamos si la predicción ya está en cache.\n",
    "Si está en cache, devolvemos el resultado almacenado.\n",
    "Si no está en cache, procesamos la imagen, realizamos la predicción y almacenamos el resultado en Redis por 1 hora (3600 segundos).\n",
    "2.2. Optimizar el Modelo para Reducir la Latencia\n",
    "Reducir el tiempo que tarda el modelo en realizar predicciones puede mejorar significativamente la experiencia del usuario.\n",
    "\n",
    "2.2.1. Aplicar Quantización al Modelo\n",
    "La quantización reduce el tamaño del modelo y acelera las inferencias sin una pérdida significativa en la precisión.\n",
    "\n",
    "Instalar TensorFlow Lite (si no está instalado):\n",
    "\n",
    "bash\n",
    "Copiar\n",
    "pip install tensorflow\n",
    "Crear un Script para Quantizar el Modelo:\n",
    "\n",
    "Crea un archivo llamado quantize_model.py en el directorio raíz de tu proyecto.\n",
    "\n",
    "python\n",
    "Copiar\n",
    "# quantize_model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Cargar el modelo existente\n",
    "model = tf.keras.models.load_model('modelo_cnn_transfer_learning_mnist_final_optimizado.h5')\n",
    "\n",
    "# Convertir el modelo a TensorFlow Lite con quantización\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Guardar el modelo quantizado\n",
    "with open('modelo_cnn_transfer_learning_mnist_quantizado.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(\"Modelo quantizado guardado como 'modelo_cnn_transfer_learning_mnist_quantizado.tflite'\")\n",
    "Ejecutar el Script de Quantización:\n",
    "\n",
    "bash\n",
    "Copiar\n",
    "python quantize_model.py\n",
    "Actualizar api.py para Utilizar el Modelo Quantizado:\n",
    "\n",
    "Modifica la carga del modelo para usar el modelo quantizado.\n",
    "\n",
    "python\n",
    "Copiar\n",
    "# api.py (modificación en la carga del modelo)\n",
    "\n",
    "# Cargar el modelo quantizado\n",
    "interpreter = tf.lite.Interpreter(model_path='modelo_cnn_transfer_learning_mnist_quantizado.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "Modificar la función de predicción:\n",
    "\n",
    "python\n",
    "Copiar\n",
    "# api.py (modificación en la función predict_web)\n",
    "\n",
    "def predict_web():\n",
    "    # ... (código anterior)\n",
    "    try:\n",
    "        # ... (código de procesamiento de imagen)\n",
    "\n",
    "        # Realizar la predicción usando TensorFlow Lite\n",
    "        interpreter.set_tensor(input_details[0]['index'], img_rgb.numpy())\n",
    "        interpreter.invoke()\n",
    "        pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "        pred_class = np.argmax(pred, axis=1)[0]\n",
    "        pred_prob = float(np.max(pred))\n",
    "\n",
    "        # Almacenar el resultado en cache por 1 hora\n",
    "        cache.setex(cache_key, 3600, f\"{pred_class},{pred_prob}\")\n",
    "\n",
    "        return jsonify({'prediccion': int(pred_class), 'probabilidad': pred_prob})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "Nota: Asegúrate de que el modelo quantizado sea compatible con TensorFlow Lite y que las dimensiones de entrada sean correctas.\n",
    "\n",
    "2.2.2. Evaluar el Impacto de la Optimización\n",
    "Después de aplicar la quantización, es importante evaluar si hubo mejoras en el rendimiento y si la precisión del modelo se mantiene aceptable.\n",
    "\n",
    "Realizar Pruebas de Rendimiento:\n",
    "\n",
    "Medir el tiempo de inferencia antes y después de la optimización.\n",
    "Utilizar herramientas como timeit o perfiles de rendimiento.\n",
    "Verificar la Precisión del Modelo:\n",
    "\n",
    "Asegurarse de que la quantización no ha reducido significativamente la precisión del modelo.\n",
    "Comparar las predicciones del modelo original y del modelo quantizado en un conjunto de datos de prueba.\n",
    "2.3. Implementar Asynchronous Processing (Opcional)\n",
    "Si la API recibe una gran cantidad de solicitudes simultáneas, procesarlas de manera asíncrona puede mejorar la capacidad de respuesta.\n",
    "\n",
    "Instalar Celery y Redis como Broker:\n",
    "\n",
    "bash\n",
    "Copiar\n",
    "pip install celery\n",
    "Configurar Celery en api.py:\n",
    "\n",
    "python\n",
    "Copiar\n",
    "# api.py (añadir configuración de Celery)\n",
    "\n",
    "from celery import Celery\n",
    "\n",
    "# Configurar Celery\n",
    "celery = Celery(\n",
    "    app.name,\n",
    "    broker=redis_url,\n",
    "    backend=redis_url\n",
    ")\n",
    "\n",
    "@celery.task\n",
    "def predict_task(file_bytes):\n",
    "    # Procesamiento de la imagen y predicción (similar a predict_web)\n",
    "    # Devuelve el resultado de la predicción\n",
    "    # ...\n",
    "    return {'prediccion': pred_class, 'probabilidad': pred_prob}\n",
    "\n",
    "@app.route('/predict_async', methods=['POST'])\n",
    "@require_api_key\n",
    "def predict_async():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No se encontró el archivo en la solicitud.'}), 400\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No se seleccionó ningún archivo.'}), 400\n",
    "    if not allowed_file(file.filename):\n",
    "        return jsonify({'error': 'Tipo de archivo no permitido. Por favor, sube una imagen PNG, JPG, JPEG o GIF.'}), 400\n",
    "    try:\n",
    "        file_bytes = file.read()\n",
    "        task = predict_task.delay(file_bytes)\n",
    "        return jsonify({'task_id': task.id}), 202\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "Ejecutar el Worker de Celery:\n",
    "\n",
    "bash\n",
    "Copiar\n",
    "celery -A api.celery worker --loglevel=info\n",
    "Crear un Endpoint para Consultar el Estado de la Tarea:\n",
    "\n",
    "python\n",
    "Copiar\n",
    "@app.route('/tasks/<task_id>', methods=['GET'])\n",
    "@require_api_key\n",
    "def get_task_status(task_id):\n",
    "    task = predict_task.AsyncResult(task_id)\n",
    "    if task.state == 'PENDING':\n",
    "        response = {\n",
    "            'state': task.state,\n",
    "            'status': 'Pendiente...'\n",
    "        }\n",
    "    elif task.state != 'FAILURE':\n",
    "        response = {\n",
    "            'state': task.state,\n",
    "            'result': task.result\n",
    "        }\n",
    "    else:\n",
    "        response = {\n",
    "            'state': task.state,\n",
    "            'status': str(task.info)  # Información sobre el error\n",
    "        }\n",
    "    return jsonify(response)\n",
    "Nota: La implementación asíncrona es opcional y depende de las necesidades de tu aplicación. Puede ser útil si esperas una alta concurrencia o tareas de predicción que tomen mucho tiempo.\n",
    "\n",
    "3. Configurar la Escalabilidad en Heroku\n",
    "A medida que aumenta el tráfico a tu API, es crucial que la aplicación pueda escalar para manejar la carga sin degradar el rendimiento.\n",
    "\n",
    "3.1. Escalar Dynos en Heroku\n",
    "Los dynos son las unidades de ejecución en Heroku. Escalar dynos permite manejar más solicitudes simultáneas.\n",
    "\n",
    "Escalar Dynos Manualmente:\n",
    "\n",
    "bash\n",
    "Copiar\n",
    "heroku ps:scale web=2 --app mnist-flask-api\n",
    "Explicación:\n",
    "\n",
    "Este comando aumenta el número de dynos web a 2, permitiendo que la aplicación maneje más tráfico.\n",
    "Verificar el Número de Dynos:\n",
    "\n",
    "bash\n",
    "Copiar\n",
    "heroku ps --app mnist-flask-api\n",
    "3.2. Configurar Auto-Scaling (si es aplicable)\n",
    "Heroku ofrece planes con capacidades de auto-scaling que ajustan automáticamente el número de dynos según la carga.\n",
    "\n",
    "Actualizar el Plan de Dynos:\n",
    "Cambia a un plan que soporte auto-scaling, como el plan Performance.\n",
    "Configurar Auto-Scaling:\n",
    "En el dashboard de Heroku, ve a la sección de Resources.\n",
    "Configura las reglas de auto-scaling según las métricas de uso (CPU, memoria, etc.).\n",
    "Nota: El auto-scaling está disponible en ciertos planes de Heroku y puede incurrir en costos adicionales.\n",
    "\n",
    "4. Mejorar la Gestión de Logs y Monitoreo\n",
    "Mantener una buena gestión de logs y un monitoreo efectivo es esencial para detectar y resolver problemas rápidamente.\n",
    "\n",
    "4.1. Refinar el Uso de Loguru\n",
    "Asegúrate de que los logs sean claros y útiles para el diagnóstico.\n",
    "\n",
    "Configurar Niveles de Logging:\n",
    "Usa diferentes niveles (INFO, WARNING, ERROR) para categorizar los mensajes.\n",
    "Integrar Loguru con Servicios de Logs Externos:\n",
    "Considera enviar logs a servicios como Papertrail, Loggly o ELK Stack para una mejor gestión y análisis.\n",
    "4.2. Mejorar el Monitoreo con Prometheus y Grafana\n",
    "Refina las métricas monitoreadas para obtener una visión más detallada del rendimiento de la API.\n",
    "\n",
    "Añadir Métricas Personalizadas:\n",
    "Monitorea métricas como el tiempo de respuesta por endpoint, tasa de errores, etc.\n",
    "Crear Dashboards Personalizados en Grafana:\n",
    "Diseña dashboards que resalten las métricas más importantes para tu aplicación.\n",
    "Configurar Alertas:\n",
    "Configura alertas para notificarte cuando ciertas métricas superen umbrales críticos (e.g., alta latencia, aumento en la tasa de errores).\n",
    "4.3. Utilizar Herramientas de Monitoreo Adicionales (Opcional)\n",
    "Considera integrar otras herramientas de monitoreo para obtener una visión más completa, como:\n",
    "\n",
    "New Relic: Para monitoreo de rendimiento de aplicaciones.\n",
    "Sentry: Para seguimiento de errores en tiempo real.\n",
    "5. Conclusiones y Recomendaciones\n",
    "5.1. Optimización del Rendimiento\n",
    "Caching con Redis: Reduce la carga del servidor y disminuye la latencia al almacenar resultados de predicciones frecuentes.\n",
    "Optimización del Modelo: La quantización y otras técnicas de optimización mejoran los tiempos de inferencia sin comprometer significativamente la precisión.\n",
    "5.2. Escalabilidad en Heroku\n",
    "Escalado de Dynos: Permite manejar incrementos en el tráfico ajustando el número de dynos según la demanda.\n",
    "Auto-Scaling: Automatiza el proceso de escalado, asegurando que la aplicación se mantenga receptiva bajo cargas variables.\n",
    "5.3. Gestión de Logs y Monitoreo\n",
    "Logs Claros y Detallados: Facilitan la identificación y resolución de problemas.\n",
    "Monitoreo Avanzado: Proporciona visibilidad en tiempo real del rendimiento de la API, permitiendo acciones proactivas ante posibles incidencias.\n",
    "Recomendaciones Adicionales\n",
    "Continuar Mejorando la Seguridad:\n",
    "Implementar autenticación más robusta como OAuth2 o JWT.\n",
    "Añadir rate limiting para prevenir abusos.\n",
    "Ampliar la Funcionalidad de la API:\n",
    "Añadir endpoints adicionales para diferentes tipos de predicciones o análisis.\n",
    "Optimizar el Pipeline de CI/CD:\n",
    "Incorporar etapas adicionales como análisis estático de código, pruebas de integración o despliegue en múltiples entornos.\n",
    "Automatizar la Actualización del Modelo:\n",
    "Crear flujos de trabajo para actualizar el modelo de machine learning con nuevos datos automáticamente, asegurando que la API siempre utilice el modelo más actualizado y preciso.\n",
    "6. Recursos Adicionales\n",
    "Redis Documentation: Redis Docs\n",
    "Celery Documentation: Celery Docs\n",
    "PyTest Documentation: PyTest Docs\n",
    "GitHub Actions Documentation: GitHub Actions Docs\n",
    "Heroku Scaling Dynos: Heroku Scaling Dynos\n",
    "Loguru Documentation: Loguru Docs\n",
    "Prometheus Documentation: Prometheus Docs\n",
    "Grafana Documentation: Grafana Docs\n",
    "TensorFlow Lite Model Optimization: TensorFlow Lite Optimization\n",
    "Celery with Flask: Celery Flask Integration\n",
    "Conclusión\n",
    "En el Día 22, has fortalecido significativamente la eficiencia y capacidad de escalado de tu API de predicción mediante la implementación de caching con Redis, la optimización del modelo de machine learning y la configuración de la escalabilidad en Heroku. Además, has mejorado la gestión de logs y el monitoreo, asegurando que tu aplicación pueda manejar incrementos en la demanda mientras mantiene un rendimiento óptimo.\n",
    "\n",
    "Pasos Clave Realizados:\n",
    "\n",
    "Optimización del Rendimiento:\n",
    "\n",
    "Implementaste caching con Redis para reducir la latencia y la carga del servidor.\n",
    "Optimizar el modelo mediante quantización para mejorar los tiempos de inferencia.\n",
    "Configuración de la Escalabilidad:\n",
    "\n",
    "Escalaste manualmente los dynos en Heroku para manejar mayor tráfico.\n",
    "Configuraste auto-scaling (si es aplicable) para ajustar automáticamente los recursos según la demanda.\n",
    "Mejora de Logs y Monitoreo:\n",
    "\n",
    "Refinaste el uso de Loguru para una mejor gestión de logs.\n",
    "Mejoraste el monitoreo con Prometheus y Grafana para una visión más detallada del rendimiento de la API.\n",
    "Recomendaciones para Continuar:\n",
    "\n",
    "Continuar Mejorando la Seguridad: Implementa métodos de autenticación más robustos y añade mecanismos de prevención contra abusos.\n",
    "Ampliar la Funcionalidad: Añade más endpoints y funcionalidades a tu API para ampliar su utilidad.\n",
    "Optimizar el Pipeline de CI/CD: Incorpora etapas adicionales y automatiza más aspectos del desarrollo y despliegue.\n",
    "Automatizar la Actualización del Modelo: Implementa flujos de trabajo para mantener tu modelo actualizado con nuevos datos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
