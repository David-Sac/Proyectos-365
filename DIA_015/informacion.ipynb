{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIA 015: Implementacion de Data Augmentation para Mejorar el Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Data Augmentation (Aumento de Datos) es una técnica que genera nuevas muestras a partir de las existentes aplicando transformaciones aleatorias. Esto ayuda a mejorar la generalización del modelo y a prevenir el overfitting (sobreajuste). En este día, implementaremos Data Augmentation en el pipeline de datos de entrenamiento y reeentrenaremos el modelo para observar mejoras en su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importación de Librerías\n",
    "# ------------------------------------\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Verificación de la Disponibilidad de GPU\n",
    "# ------------------------------------\n",
    "\n",
    "# Verificar si TensorFlow detecta una GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU(s) disponible(s): {[gpu.name for gpu in gpus]}\")\n",
    "else:\n",
    "    print(\"No hay GPU disponible. El entrenamiento se realizará en CPU, lo cual puede ser más lento.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Carga y Preprocesamiento de Datos\n",
    "# ------------------------------------\n",
    "\n",
    "# a. Cargar el dataset MNIST\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# b. Normalizar los valores de píxeles de 0-255 a 0-1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# c. Añadir la dimensión de los canales (1 para escala de grises)\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# d. Verificar la forma de los datos\n",
    "print(\"Forma de X_train:\", X_train.shape)  # (60000, 28, 28, 1)\n",
    "print(\"Forma de X_test:\", X_test.shape)    # (10000, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualización de Datos (Opcional)\n",
    "# ------------------------------------\n",
    "\n",
    "# Función para visualizar imágenes\n",
    "def mostrar_imagen(matriz, etiqueta, indice):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(matriz[indice].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Dígito: {etiqueta}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Mostrar la primera imagen del conjunto de entrenamiento\n",
    "mostrar_imagen(X_train, y_train, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Definir la Función de Preprocesamiento y Data Augmentation\n",
    "# ------------------------------------\n",
    "\n",
    "# a. Definir las capas de Data Augmentation utilizando Keras\n",
    "data_augmentation_layer = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),  # Voltear horizontal y verticalmente\n",
    "    layers.RandomRotation(0.15),                    # Rotar aleatoriamente hasta 15%\n",
    "    layers.RandomZoom(0.1),                         # Aplicar zoom aleatorio hasta un 10%\n",
    "    layers.RandomContrast(0.1),                     # Ajustar el contraste aleatoriamente\n",
    "])\n",
    "\n",
    "# b. Función para aplicar preprocesamiento\n",
    "def preprocesar_imagen(x, y):\n",
    "    \"\"\"\n",
    "    Redimensiona las imágenes a 224x224, convierte de escala de grises a RGB y normaliza.\n",
    "    \n",
    "    Args:\n",
    "        x (tf.Tensor): Imagen de entrada con forma (28, 28, 1).\n",
    "        y (tf.Tensor): Etiqueta correspondiente.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Imagen preprocesada y etiqueta.\n",
    "    \"\"\"\n",
    "    # Redimensionar la imagen a 224x224\n",
    "    x = tf.image.resize(x, [224, 224])\n",
    "    \n",
    "    # Convertir de escala de grises a RGB (duplicar el canal)\n",
    "    x = tf.image.grayscale_to_rgb(x)\n",
    "    \n",
    "    # Normalizar los valores de píxeles a [0, 1]\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# c. Función para aplicar Data Augmentation\n",
    "def aplicar_data_augmentation(x, y):\n",
    "    \"\"\"\n",
    "    Aplica técnicas de Data Augmentation a las imágenes de entrenamiento.\n",
    "    \n",
    "    Args:\n",
    "        x (tf.Tensor): Imagen preprocesada con forma (224, 224, 3).\n",
    "        y (tf.Tensor): Etiqueta correspondiente.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Imagen aumentada y etiqueta.\n",
    "    \"\"\"\n",
    "    x = data_augmentation_layer(x)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Crear los Objetos tf.data.Dataset con Data Augmentation\n",
    "# ------------------------------------\n",
    "\n",
    "# a. Crear el dataset de entrenamiento\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.map(preprocesar_imagen, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.map(aplicar_data_augmentation, num_parallel_calls=tf.data.AUTOTUNE)  # Aplicar Data Augmentation solo en entrenamiento\n",
    "train_ds = train_ds.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.AUTOTUNE)  # Reducir batch_size a 32 para CPU\n",
    "\n",
    "# b. Crear el dataset de validación (sin Data Augmentation)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val_ds = val_ds.map(preprocesar_imagen, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE)  # Reducir batch_size a 32 para CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Construcción del Modelo con Transfer Learning\n",
    "# ------------------------------------\n",
    "\n",
    "# a. Cargar el modelo VGG16 sin las capas superiores y con pesos preentrenados en ImageNet\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# b. Congelar las capas del modelo base para que no se actualicen durante el entrenamiento inicial\n",
    "base_model.trainable = False\n",
    "\n",
    "# c. Construir el modelo completo con Transfer Learning (Sin data_augmentation_layer)\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')  # 10 clases para MNIST\n",
    "])\n",
    "\n",
    "# d. Resumen del modelo\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Compilación del Modelo\n",
    "# ------------------------------------\n",
    "\n",
    "# Compilar el modelo definiendo el optimizador, la función de pérdida y las métricas\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Definir Early Stopping y ModelCheckpoint\n",
    "# ------------------------------------\n",
    "\n",
    "# Definir Early Stopping para detener el entrenamiento si la pérdida de validación no mejora\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Definir ModelCheckpoint para guardar el mejor modelo basado en la pérdida de validación\n",
    "checkpoint = ModelCheckpoint('mejor_modelo.h5', monitor='val_loss', save_best_only=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Entrenamiento del Modelo con Data Augmentation\n",
    "# ------------------------------------\n",
    "\n",
    "# Entrenar el modelo usando los datasets de entrenamiento y validación\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Evaluación del Modelo\n",
    "# ------------------------------------\n",
    "\n",
    "# Evaluar el rendimiento en el conjunto de prueba\n",
    "test_loss, test_acc = model.evaluate(val_ds, verbose=2)\n",
    "print(f'\\nPrecisión en el conjunto de prueba: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Visualización del Rendimiento del Modelo\n",
    "# ------------------------------------\n",
    "\n",
    "# Graficar precisión y pérdida durante el entrenamiento\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Precisión\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Validación')\n",
    "plt.title('Precisión durante el Entrenamiento con Data Augmentation')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "\n",
    "# Pérdida\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Validación')\n",
    "plt.title('Pérdida durante el Entrenamiento con Data Augmentation')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Guardado y Carga del Mejor Modelo\n",
    "# ------------------------------------\n",
    "\n",
    "# b. Cargar el mejor modelo guardado\n",
    "mejor_modelo = tf.keras.models.load_model('mejor_modelo.h5')\n",
    "print(\"Mejor modelo cargado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Hacer Predicciones con el Mejor Modelo Cargado\n",
    "# ------------------------------------\n",
    "\n",
    "# a. Hacer predicciones sobre el conjunto de prueba\n",
    "predicciones = mejor_modelo.predict(val_ds)\n",
    "clases_predichas = np.argmax(predicciones, axis=1)\n",
    "\n",
    "# b. Mostrar predicción para la primera imagen del conjunto de prueba\n",
    "indice = 0\n",
    "print(f\"Etiqueta real: {y_test[indice]}\")\n",
    "print(f\"Predicción: {clases_predichas[indice]}\")\n",
    "print(f\"Probabilidades: {predicciones[indice]}\")\n",
    "\n",
    "# c. Visualizar la imagen con su predicción\n",
    "def mostrar_prediccion(matriz, etiquetas, predicciones, indice):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(matriz[indice].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Real: {etiquetas[indice]}\\nPred: {clases_predichas[indice]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Mostrar la primera imagen del conjunto de prueba con su predicción\n",
    "mostrar_prediccion(X_test, y_test, clases_predichas, indice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Evaluación Más Detallada (Opcional)\n",
    "# ------------------------------------\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"Reporte de Clasificación:\\n\")\n",
    "print(classification_report(y_test, clases_predichas))\n",
    "\n",
    "# Generar la matriz de confusión\n",
    "cm = confusion_matrix(y_test, clases_predichas)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=np.unique(y_train), \n",
    "            yticklabels=np.unique(y_train))\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Realidad')\n",
    "plt.title('Matriz de Confusión con Data Augmentation')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
